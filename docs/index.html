<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">

  <title>Yuqing Du</title>
  <meta name="description" content="">

  <!-- Open Graph -->


  <!-- Bootstrap & MDB -->
  <link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet"
    integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg=="
    crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css"
    integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q=="
    crossorigin="anonymous" />

  <!-- Fonts & Icons -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"
    integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog=="
    crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css"
    integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg=="
    crossorigin="anonymous">
  <link rel="stylesheet" type="text/css"
    href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

  <!-- Code Syntax Highlighting -->
  <!-- <link rel="stylesheet" href="https://combinatronics.io/jwarby/pygments-css/master/github.css" /> -->

  <!-- Styles -->
  <link rel="shortcut icon" href="/assets/img/favicon.ico">
  <link rel="stylesheet" href="/assets/css/main.css">

  <link rel="canonical" href="/">


  <!-- MathJax -->
  <script defer type="text/javascript" id="MathJax-script"
    src="https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


</head>

<body class="fixed-top-nav ">

  <!-- Header -->

  <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light bg-white navbar-expand-sm fixed-top">
      <div class="container">

        <div class="row ml-1 ml-sm-0">
          <span class="contact-icon text-center" nav="true">
            <!-- <a href="mailto:%79%69%72%75.%63%68%65%6E@%63%6F%6C%75%6D%62%69%61.%65%64%75"><i class="fas fa-envelope"></i></a> -->
            <a href="https://scholar.google.com/citations?user=j9GCzQUAAAAJ&hl=en" target="_blank"
              title="Google Scholar"><i class="ai ai-google-scholar"></i></a>
            <a href="https://github.com/yuqingd" target="_blank" title="GitHub"><i class="fab fa-github"></i></a>
            <a href="https://www.linkedin.com/in/yuqingdu" target="_blank" title="LinkedIn"><i
                class="fab fa-linkedin"></i></a>
            <a href="https://twitter.com/d_yuqing" target="_blank" title="Twitter"><i class="fab fa-twitter"></i></a>
          </span>

        </div>

        <!-- Navbar Toogle -->
        <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav"
          aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar top-bar"></span>
          <span class="icon-bar middle-bar"></span>
          <span class="icon-bar bottom-bar"></span>
        </button>
        <div class="collapse navbar-collapse text-right" id="navbarNav">
          <ul class="navbar-nav ml-auto flex-nowrap">
            <!-- About -->
            <li class="nav-item active">
              <a class="nav-link" href="/">
                home

                <span class="sr-only">(current)</span>

              </a>
            </li>

            <!-- Other pages -->





            <li class="nav-item ">
              <a class="nav-link" href="/projects/">


              </a>
            </li>



            <li class="nav-item ">
              <a class="nav-link" href="/publications/">


              </a>
            </li>



            <li class="nav-item ">
              <a class="nav-link" href="/teaching/">


              </a>
            </li>










          </ul>
        </div>
      </div>
    </nav>

  </header>


  <!-- Content -->

  <div class="container mt-5">
    <div class="post">

      <header class="post-header">
        <h1 class="post-title">
          <span class="font-weight-bold">Yuqing</span> Du
        </h1>
      </header>

      <article>

        <div class="profile float-right">

          <img class="img-fluid z-depth-1 rounded" src="/assets/img/prof_pic.jpg">

          <div class="address">
            <p><small>yuqing_du (at) berkeley (dot) edu </small> </p>

          </div>

        </div>


        <div class="clearfix">
          <p>Hello! I am a Research Scientist at Google DeepMind. Currently I work on image (<a href="https://deepmind.google/technologies/imagen-3/">Imagen</a>) 
            and video (<a href="https://deepmind.google/technologies/veo/veo-2/">Veo</a>) generation. I am broadly interested in enabling intelligent 
          systems to learn from and with humans.</p>
          
          <p></p>I received my PhD from UC Berkeley, advised by
            Professor <a href="https://people.eecs.berkeley.edu/~pabbeel/">Pieter Abbeel</a> at the <a
              href="http://bair.berkeley.edu/">Berkeley Artificial Intelligence Research (BAIR) Lab</a>. My thesis,
              <a href="https://escholarship.org/content/qt6h96s744/qt6h96s744.pdf">"Human-Centric Reward Design"</a>, focuses on
              <b> enabling reinforcement learning (RL) agents to learn from humans via novel reward design methods
                 that incorporate human priors, knowledge, and direct input</b>. 
              I was also
               a visiting researcher at <a href="https://ai.meta.com/research/">FAIR
                (Meta)</a>, where I worked on using RL for improving reasoning in LLMs,
                 and an intern at <a href="https://www.deepmind.com">DeepMind</a> and <a
                href="https://x.company/projects/everyday-robots/">Everyday Robots</a>, where I worked on improving 
                imitation learning for robotics. </p>    
          
          Before that, I received a B.A.Sc in Engineering Physics with a minor in Honours Mathematics from the University of British
          Columbia, where I was advised by Professors <a
            href="http://caris-mech.sites.olt.ubc.ca/profiles/machiel-van-der-loos/">Machiel Van der Loos</a> and <a
            href="https://research.monash.edu/en/persons/elizabeth-croft">Elizabeth Croft</a> at the <a
            href="https://caris.mech.ubc.ca/">Collaborative Advanced Robotics and Intelligent Systems (CARIS) Lab</a>,
          where I worked on human-robot interaction.</p>

          <!--
Write your biography here. Tell the world about yourself. Link to your favorite [subreddit](http://reddit.com){:target="\_blank"}. You can put a picture in, too. The code is already in, just name your picture `prof_pic.jpg` and put it in the `img/` folder.

Put your address / P.O. box / other info right below your picture. You can also disable any these elements by editing `profile` property of the YAML header of your `_pages/about.md`. Edit `_bibliography/papers.bib` and Jekyll will render your [publications page](/al-folio/publications/) automatically.

Link to your social media connections, too. This theme is set up to use [Font Awesome icons](http://fortawesome.github.io/Font-Awesome/){:target="\_blank"} and [Academicons](https://jpswalsh.github.io/academicons/){:target="\_blank"}, like the ones below. Add your Facebook, Twitter, LinkedIn, Google Scholar, or just disable all of them. -->

        </div>




        <div class="publications">
          <h2>papers</h2>
          <ol class="bibliography">

            <li>
              <div class="row">
                <div class="col-sm-4 float-left">

                  <center>
                    <img class="img-fluid z-depth-1 rounded" src="/assets/img/dynalang.png">
                  </center>
                </div>

                <div id="lin2023dynalang" class="col-sm-7">

                  <div class="title"><a href="https://arxiv.org/pdf/2308.01399.pdf">Learning to Model the World with
                      Language</a></div>
                  <div class="author">
                    <a href="https://www.jessylin.com/"> Jessy Lin</a>,
                    <em>Yuqing Du</em>,
                    <a href="https://www.linkedin.com/in/olivia-watkins-a73580bb/"> Olivia Watkins</a>,
                    <a href="https://danijar.com/">Danijar Hafner</a>,
                    <a href="https://people.eecs.berkeley.edu/~pabbeel/"> Pieter Abbeel</a>,
                    <a href="https://people.eecs.berkeley.edu/~klein/"> Dan Klein</a>,
                    and <a href="http://people.eecs.berkeley.edu/~anca/" target="_blank">Anca Dragan</a>


                  </div>

                  <div class="periodical">

                    <em>International Conference on Machine Learning (ICML) 2024</em>  <br><b>(Oral)</b>

                  </div>


                  <div class="links">

                    <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
                    <a href="https://arxiv.org/abs/2308.01399" class="btn btn-sm z-depth-0" role="button"
                      target="_blank">arXiv</a>
                    <a href="https://dynalang.github.io" class="btn btn-sm z-depth-0" role="button"
                      target="_blank">Website</a>
                    <a href="https://github.com/jlin816/dynalang" class="btn btn-sm z-depth-0" role="button"
                      target="_blank">Code</a>

                  </div>

                  <!-- Hidden abstract block -->

                  <div class="abstract hidden">
                    <p>To interact with humans in the world, agents need to understand the diverse types of language
                      that people use, relate them to the visual world, and act based on them. While current agents
                      learn to execute simple language instructions from task rewards, we aim to build agents that
                      leverage diverse language that conveys general knowledge, describes the state of the world,
                      provides interactive feedback, and more. Our key idea is that language helps agents predict the
                      future: what will be observed, how the world will behave, and which situations will be rewarded.
                      This perspective unifies language understanding with future prediction as a powerful
                      self-supervised learning objective. We present Dynalang, an agent that learns a multimodal world
                      model that predicts future text and image representations and learns to act from imagined model
                      rollouts. Unlike traditional agents that use language only to predict actions, Dynalang acquires
                      rich language understanding by using past language also to predict future language, video, and
                      rewards. In addition to learning from online interaction in an environment, Dynalang can be
                      pretrained on datasets of text, video, or both without actions or rewards. From using language
                      hints in grid worlds to navigating photorealistic scans of homes, Dynalang utilizes diverse types
                      of language to improve task performance, including environment descriptions, game rules, and
                      instructions. </p>
                  </div>

                </div>


              </div>
            </li>

            <li>
              <div class="row">
                <div class="col-sm-4 float-left">

                  <!-- <center>
                    <img class="img-fluid z-depth-1 rounded" src="/assets/img/dynalang.png">
                  </center> -->
                </div>

                <div id="havrilla2024reasoning" class="col-sm-7">

                  <div class="title"><a href="https://arxiv.org/pdf/2403.04642">Teaching Large Language Models to Reason
                    with Reinforcement Learning</a></div>
                  <div class="author">
                    <a href="https://dahoas.github.io"> Alexander Havrilla</a>,
                    <em>Yuqing Du</em>,
                    <a href="https://sharathraparthy.github.io"> Sharath Chandra Raparthy</a>,
                    <a href="https://scholar.google.com/citations?user=1Z4PmxIAAAAJ&hl=en">  Christoforos Nalmpantis</a>,
                    <a href="https://ai.meta.com/people/347567754347428/jane-yu/"> Jane Dwivedi-Yu</a>,
                    <a href="https://scholar.google.com/citations?user=BLXPkDEAAAAJ&hl=en"> Maksym Zhuravinskyi</a>,
                    <a href="https://erichambro.com"> Eric Hambro</a>,
                    <a href="https://tesatory.github.io"> Sainbayar Sukhbaatar</a>,
                    and <a href="https://rraileanu.github.io">Roberta Raileanu</a>


                  </div>

                  <div class="periodical">

                    <em>AI4MATH Workshop @ ICML 2024 </em>

                  </div>


                  <div class="links">

                    <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
                    <a href="https://arxiv.org/pdf/2403.04642" class="btn btn-sm z-depth-0" role="button"
                      target="_blank">arXiv</a>

                  </div>

                  <!-- Hidden abstract block -->

                  <div class="abstract hidden">
                    <p>Reinforcement Learning from Human Feedback (RLHF) has emerged as a dominant approach for align-
                      ing LLM outputs with human preferences. Inspired by the success of RLHF, we study the performance
                      of multiple algorithms that learn from feedback (Expert Iteration, Proximal Policy Optimization
                      (PPO), Return-Conditioned RL) on improving LLM reasoning capabilities. We investigate both sparse
                      and dense rewards provided to the LLM both heuristically and via a learned reward model. We
                      additionally start from multiple model sizes and initializations both with and without supervised
                      fine-tuning (SFT) data. Overall, we find all algorithms perform comparably, with Expert Iteration
                      performing best in most cases. Surprisingly, we find the sample complexity of Expert Iteration is
                      similar to that of PPO, requiring at most on the order of 106 samples to converge from a pretrained
                      checkpoint. We investigate why this is the case, concluding that during RL training models fail to
                      explore significantly beyond solutions already produced by SFT models. Additionally, we discuss a
                      trade off between maj@1 and pass@96 metric performance during SFT training and how conversely
                      RL training improves both simultaneously. We then conclude by discussing the implications of our
                      findings for RLHF and the future role of RL in LLM fine-tuning.</p>
                  </div>

                </div>


              </div>
            </li>

            <li>
              <div class="row">
                <div class="col-sm-4 float-left">

                  <center>
                    <img class="img-fluid z-depth-1 rounded" src="/assets/img/exploration.png">
                  </center>
                </div>

                <div id="du2023exploration" class="col-sm-7">

                  <div class="title"><a href="https://openreview.net/pdf?id=5uU9TY2oME">What can AI Learn from Human Exploration?
                    Intrinsically-Motivated Humans and Agents in Open-World Exploration</a></div>
                  <div class="author">
                    <em>Yuqing Du*</em>,
                    <a href="https://www.elizakosoy.com"> Eliza Kosoy*</a>,
                    <a href="https://alyd.github.io">  Alyssa L. Dayan*</a>,
                    <a href="https://www.linkedin.com/in/mariarufova"> Maria Rufova</a>,
                    <a href="https://people.eecs.berkeley.edu/~pabbeel/"> Pieter Abbeel</a>,
                    and <a href="http://alisongopnik.com">Alison Gopnik</a>


                  </div>

                  <div class="periodical">

                    <em> Intrinsically Motivated Open-Ended Learning Workshop @ NeurIPS 2023 </em>  <br><b>(Spotlight)</b>

                  </div>


                  <div class="links">

                    <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
                    <a href="https://openreview.net/pdf?id=5uU9TY2oME" class="btn btn-sm z-depth-0" role="button"
                      target="_blank">PDF</a>

                  </div>

                  <!-- Hidden abstract block -->

                  <div class="abstract hidden">
                    <p>What drives exploration? Understanding intrinsic motivation is a long-standing
                      question in both cognitive science and artificial intelligence (AI); numerous ex-
                      ploration objectives have been proposed and tested in human experiments and
                      used to train reinforcement learning (RL) agents. However, experiments in the
                      former are often in simplistic environments that do not capture the complexity
                      of real world exploration. On the other hand, experiments in the latter use more
                      complex environments, yet the trained RL agents fail to come close to human
                      exploration efficiency. To study this gap, we propose a framework for directly com-
                      paring human and agent exploration in an open-ended environment, Crafter [23].
                      We study how well commonly-proposed information theoretic intrinsic objectives
                      relate to actual human and agent behaviours, finding that human and intrinsically-
                      motivated RL agent exploration success consistently show positive correlation with
                      Entropy and Empowerment. However, only human exploration shows significant
                      correlation with Information Gain. In a preliminary analysis of verbalizations, we
                      find that children’s verbalizations of goals show a strong positive correlation with
                      Empowerment, suggesting that goal-setting may be an important aspect of efficient
                      exploration. </p>
                  </div>

                </div>


              </div>
            </li>

            <li>
              <div class="row">
                <div class="col-sm-4 float-left">

                  <!-- <center>
                    <img class="img-fluid z-depth-1 rounded" src="/assets/img/reasoning.png">
                  </center> -->
                </div>

                <div id="du2023astudy" class="col-sm-7">

                  <div class="title"><a href="https://openreview.net/pdf?id=tCZFmDyPFm">A Study on Improving Reasoning in Language Models</a></div>
                  <div class="author">
                    <em>Yuqing Du</em>,
                    <a href="https://dahoas.github.io"> Alexander Havrilla</a>,
                    <a href="https://tesatory.github.io"> Sainbayar Sukhbaatar</a>,
                    <a href="https://people.eecs.berkeley.edu/~pabbeel/"> Pieter Abbeel</a>,
                    and <a href="https://rraileanu.github.io">Roberta Raileanu</a>


                  </div>

                  <div class="periodical">

                    <em>I Can’t Believe It’s Not Better! (ICBINB) Workshop @ NeurIPS 2023 </em>

                  </div>


                  <div class="links">

                    <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
                    <a href="https://openreview.net/pdf?id=tCZFmDyPFm" class="btn btn-sm z-depth-0" role="button"
                      target="_blank">PDF</a>

                  </div>

                  <!-- Hidden abstract block -->

                  <div class="abstract hidden">
                    <p>Accurately carrying out complex reasoning is a crucial component of deployable
                      and reliable language models. While current language models can exhibit this
                      capability with few-shot guidance, accurate reasoning is primarily restricted to
                      larger model sizes. In this work, we explore methods for improving the reasoning
                      capabilities of smaller language models which are more deployable than their larger
                      counterparts. Specifically, we look at variations of supervised learning, online
                      reinforcement learning with PPO, and distillation from larger models. Surprisingly,
                      for reasoning tasks such as CommonsenseQA and GSM8K we find that
                      simple filtered supervised learning often outperforms reward-conditioned super-
                      vised learning, and that simple iterative supervised learning performs on par with
                      online reinforcement learning. </p>
                  </div>

                </div>


              </div>
            </li>

            <li>
              <div class="row">
                <div class="col-sm-4 float-left">
                  <center>
                    <img class="img-fluid z-depth-1 rounded" src="/assets/img/dpok.png">
                    <!-- <abbr class="badge">CoLLAs 2023</abbr> -->
                  </center>
                </div>

                <div id="fan2023dpok" class="col-sm-7">

                  <div class="title"><a href="https://arxiv.org/pdf/2305.16381.pdf">DPOK: Reinforcement Learning for
                      Fine-tuning Text-to-Image Diffusion Models</a></div>
                  <div class="author">
                    <a href="https://pages.cs.wisc.edu/~yingfan/">Ying Fan</a>,
                    <a href="https://www.linkedin.com/in/olivia-watkins-a73580bb/"> Olivia Watkins</a>,
                    <em>Yuqing Du</em>,
                    <a href="https://www.haoliu.site"> Hao Liu</a>,
                    <a href="https://www.linkedin.com/in/moonkyung-ryu-b1199918/"> Moonkyung Ryu</a>,
                    <a href="https://www.cs.toronto.edu/~cebly/"> Craig Boutilier</a>,
                    <a href="https://people.eecs.berkeley.edu/~pabbeel/"> Pieter Abbeel</a>,
                    <a href="https://mohammadghavamzadeh.github.io"> Mohammad Ghavamzadeh</a>,
                    <a href="https://kangwooklee.com"> Kangwook Lee</a>, and
                    <a href="https://sites.google.com/view/kiminlee"> Kimin Lee</a>

                  </div>

                  <div class="periodical">

                    <em>Neural Information Processing Systems (NeurIPS) 2023</em>

                  </div>


                  <div class="links">
                    <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
                    <a href="https://arxiv.org/abs/2305.16381" class="btn btn-sm z-depth-0" role="button"
                      target="_blank">arXiv</a>
                    <a href="https://sites.google.com/view/dpok-t2i-diffusion/home" class="btn btn-sm z-depth-0"
                      role="button" target="_blank">Website</a>
                  </div>

                  <!-- Hidden abstract block -->

                  <div class="abstract hidden">
                    <p>Learning from human feedback has been shown to improve text-to-image models. These techniques
                      first learn a reward function that captures what humans care about in the task and then improve
                      the models based on the learned reward function. Even though relatively simple approaches (e.g.,
                      rejection sampling based on reward scores) have been investigated, fine-tuning text-to-image
                      models with the reward function remains challenging. In this work, we propose using online
                      reinforcement learning (RL) to fine-tune text-to-image models. We focus on diffusion models,
                      defining the fine-tuning task as an RL problem, and updating the pre-trained text-to-image
                      diffusion models using policy gradient to maximize the feedback-trained reward. Our approach,
                      coined DPOK, integrates policy optimization with KL regularization. We conduct an analysis of KL
                      regularization for both RL fine-tuning and supervised fine-tuning. In our experiments, we show
                      that DPOK is generally superior to supervised fine-tuning with respect to both image-text
                      alignment and image quality. </p>
                  </div>

                </div>
              </div>
            </li>

            <li>
              <div class="row">
                <div class="col-sm-4 float-left">
                  <center>
                    <img class="img-fluid z-depth-1 rounded" src="/assets/img/vlm.png">
                    <!-- <abbr class="badge">CoLLAs 2023</abbr> -->
                  </center>
                </div>

                <div id="du2023vlm" class="col-sm-7">

                  <div class="title"><a href="https://arxiv.org/pdf/2303.07280.pdf">Vision-Language Models as Success
                      Detectors</a></div>
                  <div class="author">
                    <em>Yuqing Du</em>,
                    <a href="https://ksenia.konyushkova.com">Ksenia Konyushkova</a>,
                    <a href="https://mdenil.com">Misha Denil</a>,
                    <a href="https://www.akhilr.com">Akhil Raju</a>,
                    Jessica Landon,
                    <a href="https://fh295.github.io">Felix Hill</a>,
                    <a href="https://www.cs.ox.ac.uk/people/nando.defreitas/">Nando de Freitas</a>, and
                    <a href="https://scholar.google.com/citations?user=l-HhJaUAAAAJ&hl=en">Serkan Cabi</a>

                  </div>

                  <div class="periodical">

                    <em> Conference on Lifelong Learning Agents (CoLLAs) 2023</em> <br><b>(Oral)</b>

                  </div>


                  <div class="links">

                    <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
                    <a href="https://arxiv.org/abs/2303.07280" class="btn btn-sm z-depth-0" role="button"
                      target="_blank">arXiv</a>
                  </div>

                  <!-- Hidden abstract block -->

                  <div class="abstract hidden">
                    <p>Detecting successful behaviour is crucial for training intelligent agents. As such, generalisable
                      reward models are a prerequisite for agents that can learn to generalise their behaviour. In this
                      work we focus on developing robust success detectors that leverage large, pretrained
                      vision-language models (Flamingo, Alayrac et al. (2022)) and human reward annotations. Concretely,
                      we treat success detection as a visual question answering (VQA) problem, denoted SuccessVQA. We
                      study success detection across three vastly different domains: (i) interactive
                      language-conditioned agents in a simulated household, (ii) real world robotic manipulation, and
                      (iii) "in-the-wild" human egocentric videos. We investigate the generalisation properties of a
                      Flamingo-based success detection model across unseen language and visual changes in the first two
                      domains, and find that the proposed method is able to outperform bespoke reward models in
                      out-of-distribution test scenarios with either variation. In the last domain of "in-the-wild"
                      human videos, we show that success detection on unseen real videos presents an even more
                      challenging generalisation task warranting future work. We hope our initial results encourage
                      further work in real world success detection and reward modelling.</p>
                  </div>

                </div>
              </div>
            </li>


            <li>
              <div class="row">
                <div class="col-sm-4 float-left">

                  <center>
                    <img class="img-fluid z-depth-1 rounded" src="/assets/img/t2i.png">
                  </center>
                </div>

                <div id="lee2023t2i" class="col-sm-7">

                  <div class="title"><a href="https://arxiv.org/pdf/2302.12192.pdf">Aligning Text-to-Image Models using
                      Human Feedback</a></div>
                  <div class="author">
                    <a href="https://sites.google.com/view/kiminlee"> Kimin Lee</a>,
                    <a href="https://www.haoliu.site"> Hao Liu</a>,
                    <a href="https://www.linkedin.com/in/moonkyung-ryu-b1199918/"> Moonkyung Ryu</a>,
                    <a href="https://www.linkedin.com/in/olivia-watkins-a73580bb/"> Olivia Watkins</a>,
                    <em>Yuqing Du</em>,
                    <a href="https://www.cs.toronto.edu/~cebly/"> Craig Boutilier</a>,

                    <a href="https://people.eecs.berkeley.edu/~pabbeel/"> Pieter Abbeel</a>,
                    <a href="https://mohammadghavamzadeh.github.io"> Mohammad Ghavamzadeh</a>, and
                    <a href="https://sites.google.com/view/gugurus/home"> Shixiang Shane Gu</a>



                  </div>

                  <div class="periodical">

                  </div>


                  <div class="links">

                    <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
                    <a href="https://arxiv.org/abs/2302.12192" class="btn btn-sm z-depth-0" role="button"
                      target="_blank">arXiv</a>

                  </div>

                  <!-- Hidden abstract block -->

                  <div class="abstract hidden">
                    <p>Deep generative models have shown impressive results in text-to-image synthesis. However, current
                      text-to-image models often generate images that are inadequately aligned with text prompts. We
                      propose a fine-tuning method for aligning such models using human feedback, comprising three
                      stages. First, we collect human feedback assessing model output alignment from a set of diverse
                      text prompts. We then use the human-labeled image-text dataset to train a reward function that
                      predicts human feedback. Lastly, the text-to-image model is fine-tuned by maximizing
                      reward-weighted likelihood to improve image-text alignment. Our method generates objects with
                      specified colors, counts and backgrounds more accurately than the pre-trained model. We also
                      analyze several design choices and find that careful investigations on such design choices are
                      important in balancing the alignment-fidelity tradeoffs. Our results demonstrate the potential for
                      learning from human feedback to significantly improve text-to-image models. </p>
                  </div>

                </div>


              </div>
            </li>

            <li>
              <div class="row">
                <div class="col-sm-4 float-left">

                  <center>
                    <img class="img-fluid z-depth-1 rounded" src="/assets/img/ellm.png">
                    <!-- <abbr class="badge">ICML 2023</abbr> -->
                  </center>
                </div>

                <div id="du2023ellm" class="col-sm-7">

                  <div class="title"><a href="https://arxiv.org/pdf/2302.06692.pdf">Guiding Pretraining in Reinforcement
                      Learning with Large Language Models</a></div>
                  <div class="author">
                    <em>Yuqing Du*</em>,
                    <a href="https://www.linkedin.com/in/olivia-watkins-a73580bb/" target="_blank">Olivia Watkins*</a>,
                    <a href="https://zihanwangki.github.io">Zihan Wang</a>,
                    <a href="https://scholar.google.fr/citations?user=VBz8gZ4AAAAJ&hl=fr">Cédric Colas</a>,
                    <a href="https://people.eecs.berkeley.edu/~trevor/" target="_blank">Trevor Darrell</a>,
                    <a href="https://people.eecs.berkeley.edu/~pabbeel/" target="_blank">Pieter Abbeel</a>,
                    <a href="https://abhishekunique.github.io">Abhishek Gupta</a>, and
                    <a href="https://www.mit.edu/~jda/">Jacob Andreas</a>
                  </div>

                  <div class="periodical">

                    <em> International Conference on Machine Learning (ICML) 2023</em>

                  </div>


                  <div class="links">

                    <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
                    <a href="https://arxiv.org/abs/2302.06692" class="btn btn-sm z-depth-0" role="button"
                      target="_blank">arXiv</a>
                    <a href="https://github.com/yuqingd/ellm" class="btn btn-sm z-depth-0" role="button"
                      target="_blank">Code</a>

                  </div>

                  <!-- Hidden abstract block -->

                  <div class="abstract hidden">
                    <p>Reinforcement learning algorithms typically struggle in the absence of a dense, well-shaped
                      reward function. Intrinsically motivated exploration methods address this limitation by rewarding
                      agents for visiting novel states or transitions, but these methods offer limited benefits in large
                      environments where most discovered novelty is irrelevant for downstream tasks. We describe a
                      method that uses background knowledge from text corpora to shape exploration. This method, called
                      ELLM (Exploring with LLMs) rewards an agent for achieving goals suggested by a language model
                      prompted with a description of the agent's current state. By leveraging large-scale language model
                      pretraining, ELLM guides agents toward human-meaningful and plausibly useful behaviors without
                      requiring a human in the loop. We evaluate ELLM in the Crafter game environment and the Housekeep
                      robotic simulator, showing that ELLM-trained agents have better coverage of common-sense behaviors
                      during pretraining and usually match or improve performance on a range of downstream tasks.</p>
                  </div>

                </div>
              </div>
            </li>

            <li>
              <div class="row">
                <div class="col-sm-4 float-left">

                  <center>
                    <img class="img-fluid z-depth-1 rounded" src="/assets/img/tcl.png">
                    <!-- <abbr class="badge">ICRA 2023</abbr> -->
                  </center>
                </div>

                <div id="khansari2023tcl" class="col-sm-7">

                  <div class="title"><a href="https://arxiv.org/pdf/2202.01862.pdf">Practical Visual Deep Imitation
                      Learning via Task-Level Domain Consistency</a></div>
                  <div class="author">
                    <a href="https://scholar.google.ch/citations?user=Z3dxz9IAAAAJ" target="_blank">Mohi Khansari</a>,
                    <a href="https://scholar.google.com/citations?hl=en&user=i05Kw5cAAAAJ" target="_blank">Daniel
                      Ho</a>,
                    <em>Yuqing Du</em>,
                    Armando Fuentes,
                    Matthew Bennice,
                    Nicolas Sievers,
                    <a href="https://kirmani.io">Sean Kirmani</a>,
                    <a href="https://www.yunfei-bai.com" target="_blank">Yunfei Bai</a>,
                    and <a href="https://evjang.com" target="_blank">Eric Jang</a>

                  </div>

                  <div class="periodical">

                    <em> International Conference on Robotics and Automation (ICRA) 2023</em>

                  </div>


                  <div class="links">

                    <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
                    <a href="https://arxiv.org/abs/2202.01862" class="btn btn-sm z-depth-0" role="button"
                      target="_blank">arXiv</a>
                  </div>

                  <!-- Hidden abstract block -->

                  <div class="abstract hidden">
                    <p>Recent work in visual end-to-end learning for robotics has shown the promise of imitation
                      learning across a variety of tasks. Such approaches are however expensive both because they
                      require large amounts of real world data and rely on time-consuming real-world evaluations to
                      identify the best model for deployment. These challenges can be mitigated by using simulation
                      evaluations to identify high performing policies. However, this introduces the well-known “reality
                      gap” problem, where simulator inaccuracies decorrelate performance in simulation from that of
                      reality. In this paper, we build on top of prior work in GAN-based domain adaptation and introduce
                      the notion of a Task Consistency Loss (TCL), a self-supervised loss that encourages sim and real
                      alignment both at the feature and action-prediction levels. We demonstrate the effectiveness of
                      our approach by teaching a 9-DoF mobile manipulator to perform the challenging task of latched
                      door opening purely from visual inputs such as RGB and depth images. We achieve 69% success across
                      twenty seen and unseen meeting rooms using only ~ 16.2 hours of teleoperated demonstrations in sim
                      and real. To the best of our knowledge, this is the first work to tackle latched door opening from
                      a purely end-to-end learning approach, where the task of navigation and manipulation are jointly
                      modeled by a single neural network.</p>
                  </div>

                </div>
              </div>
            </li>


            <li>
              <div class="row">
                <div class="col-sm-4 float-left">

                  <center>
                    <img class="img-fluid z-depth-1 rounded" src="/assets/img/vib.png">
                    <!-- <abbr class="badge">ICML 2022</abbr> -->
                  </center>
                </div>

                <div id="du2022vib" class="col-sm-7">

                  <div class="title"><a href="https://arxiv.org/pdf/2202.07600.pdf">Bayesian Imitation Learning for
                      End-to-End Mobile Manipulation</a></div>
                  <div class="author">
                    <em>Yuqing Du</em>,
                    <a href="https://scholar.google.com/citations?hl=en&user=i05Kw5cAAAAJ" target="_blank">Daniel
                      Ho</a>,
                    <a href="https://www.alexalemi.com" target="_blank">Alexander A. Alemi</a>,
                    <a href="https://evjang.com" target="_blank">Eric Jang</a>,
                    and <a href="https://scholar.google.ch/citations?user=Z3dxz9IAAAAJ" target="_blank">Mohi
                      Khansari</a>

                  </div>

                  <div class="periodical">

                    <em>International Conference on Machine Learning (ICML) 2022</em>

                  </div>


                  <div class="links">

                    <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
                    <a href="https://arxiv.org/abs/2202.07600" class="btn btn-sm z-depth-0" role="button"
                      target="_blank">arXiv</a>
                  </div>

                  <!-- Hidden abstract block -->

                  <div class="abstract hidden">
                    <p>In this work we investigate and demonstrate benefits of a Bayesian approach to imitation learning
                      from multiple sensor inputs, as applied to the task of opening office doors with a mobile
                      manipulator. Augmenting policies with additional sensor inputs, such as RGB + depth cameras, is a
                      straightforward approach to improving robot perception capabilities, especially for tasks that may
                      favor different sensors in different situations. As we scale multi-sensor robotic learning to
                      unstructured real-world settings (e.g. offices, homes) and more complex robot behaviors, we also
                      increase reliance on simulators for cost, efficiency, and safety. Consequently, the sim-to-real
                      gap across multiple sensor modalities also increases, making simulated validation more difficult.
                      We show that using the Variational Information Bottleneck (Alemi et al., 2016) to regularize
                      convolutional neural networks improves generalization to held-out domains and reduces the
                      sim-to-real gap in a sensor-agnostic manner. As a side effect, the learned embeddings also provide
                      useful estimates of model uncertainty for each sensor. We demonstrate that our method is able to
                      help close the sim-to-real gap and successfully fuse RGB and depth modalities based on
                      understanding of the situational uncertainty of each sensor. In a real-world office environment,
                      we achieve 96% task success, improving upon the baseline by +16%.</p>
                  </div>

                </div>
              </div>
            </li>




            <li>
              <div class="row">
                <div class="col-sm-4 float-left">

                  <center>
                    <img class="img-fluid z-depth-1 rounded" src="/assets/img/cusp.png">
                    <!-- <abbr class="badge">ICLR 2022</abbr> -->
                  </center>
                </div>

                <div id="du2022tango" class="col-sm-7">

                  <div class="title"><a href="https://arxiv.org/pdf/2202.10608.pdf">It Takes Four to Tango: Multiagent
                      Selfplay for Automatic Curriculum Generation</a></div>
                  <div class="author">
                    <em>Yuqing Du</em>,
                    <a href="https://people.eecs.berkeley.edu/~pabbeel/" target="_blank">Pieter Abbeel</a>,
                    and <a href="http://aditya-grover.github.io/" target="_blank">Aditya Grover</a>

                  </div>

                  <div class="periodical">

                    <em>International Conference on Learning Representations (ICLR) 2022</em>

                  </div>


                  <div class="links">

                    <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>


                    <a href="https://arxiv.org/abs/2202.10608" class="btn btn-sm z-depth-0" role="button"
                      target="_blank">arXiv</a>
                    <a href="https://github.com/yuqingd/cusp" class="btn btn-sm z-depth-0" role="button"
                      target="_blank">Code</a>
                  </div>

                  <!-- Hidden abstract block -->

                  <div class="abstract hidden">
                    <p>We are interested in training general-purpose reinforcement learning agents that can solve a wide
                      variety of goals. Training such agents efficiently requires automatic generation of a goal
                      curriculum. This is challenging as it requires (a) exploring goals of increasing difficulty, while
                      ensuring that the agent (b) is exposed to a diverse set of goals in a sample efficient manner and
                      (c) does not catastrophically forget previously solved goals. We propose Curriculum Self Play
                      (CuSP), an automated goal generation framework that seeks to satisfy these desiderata by virtue of
                      a multi-player game with 4 agents. We extend the asymmetric curricula learning in PAIRED (Dennis
                      et al., 2020) to a symmetrized game that carefully balances cooperation and competition between
                      two off-policy student learners and two regret-maximizing teachers. CuSP additionally introduces
                      entropic goal coverage and accounts for the non-stationary nature of the students, allowing us to
                      automatically induce a curriculum that balances progressive exploration with anti-catastrophic
                      exploitation. We demonstrate that our method succeeds at generating an effective curricula of
                      goals for a range of control tasks, outperforming other methods at zero-shot test-time
                      generalization to novel out-of-distribution goals. </p>
                  </div>

                </div>
              </div>
            </li>



            <li>
              <div class="row">
                <div class="col-sm-4 float-left">

                  <center>
                    <img class="img-fluid z-depth-1 rounded" src="/assets/img/sim2real.png">
                    <!-- <abbr class="badge">ICRA 2021</abbr> -->
                  </center>
                </div>

                <div id="du2020sim2real" class="col-sm-7">

                  <div class="title"><a href="https://arxiv.org/pdf/2104.07662.pdf">Auto-Tuned Sim-to-Real Transfer</a>
                  </div>
                  <div class="author">
                    <em>Yuqing Du*</em>,
                    <a href="https://www.linkedin.com/in/olivia-watkins-a73580bb/" target="_blank">Olivia Watkins*</a>,
                    <a href="https://people.eecs.berkeley.edu/~trevor/" target="_blank">Trevor Darrell</a>,
                    <a href="https://people.eecs.berkeley.edu/~pabbeel/" target="_blank">Pieter Abbeel</a>,
                    and <a href="https://www.cs.cmu.edu/~dpathak/" target="_blank">Deepak Pathak</a>

                  </div>

                  <div class="periodical">

                    <em>International Conference on Robotics and Automation (ICRA) 2021</em> <br><b>(Best Cognitive
                      Robotics Paper Finalist)</b>

                  </div>


                  <div class="links">

                    <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>


                    <a href="https://arxiv.org/abs/2104.07662" class="btn btn-sm z-depth-0" role="button"
                      target="_blank">arXiv</a>
                    <a href="https://yuqingd.github.io/autotuned-sim2real/" class="btn btn-sm z-depth-0" role="button"
                      target="_blank">Website</a>
                    <a href="https://github.com/yuqingd/sim2real2sim_rad" class="btn btn-sm z-depth-0" role="button"
                      target="_blank">Code</a>
                  </div>

                  <!-- Hidden abstract block -->

                  <div class="abstract hidden">
                    <p>Policies trained in simulation often fail when transferred to the real world due to the `reality
                      gap' where the simulator is unable to accurately capture the dynamics and visual properties of the
                      real world. Current approaches to tackle this problem, such as domain randomization, require prior
                      knowledge and engineering to determine how much to randomize system parameters in order to learn a
                      policy that is robust to sim-to-real transfer while also not being too conservative. We propose a
                      method for automatically tuning simulator system parameters to match the real world using only raw
                      RGB images of the real world without the need to define rewards or estimate state. Our key insight
                      is to reframe the auto-tuning of parameters as a search problem where we iteratively shift the
                      simulation system parameters to approach the real-world system parameters. We propose a Search
                      Param Model (SPM) that, given a sequence of observations and actions and a set of system
                      parameters, predicts whether the given parameters are higher or lower than the true parameters
                      used to generate the observations. We evaluate our method on multiple robotic control tasks in
                      both sim-to-sim and sim-to-real transfer, demonstrating significant improvement over naive domain
                      randomization. </p>
                  </div>

                </div>
              </div>
            </li>
            <li>
              <div class="row">
                <div class="col-sm-4 float-left">

                  <center>
                    <img class="img-fluid z-depth-1 rounded" src="/assets/img/empowerment.png">
                    <!-- <abbr class="badge">NeurIPS 2020</abbr> -->
                  </center>
                </div>

                <div id="du2020ave" class="col-sm-7">

                  <div class="title"><a href="http://arxiv.org/pdf/2006.14796.pdf">AvE: Assistance via Empowerment</a>
                  </div>
                  <div class="author">
                    <em>Yuqing Du</em>,
                    <a href="https://www.researchgate.net/profile/Stas_Tiomkin" target="_blank">Stas Tiomkin</a>,
                    <a href="https://www.microsoft.com/en-us/research/people/emrek/" target="_blank">Emre Kiciman</a>,
                    <a href="https://scholar.google.com/citations?user=wdZT8N8AAAAJ&hl=en" target="_blank">Daniel
                      Polani</a>,
                    <a href="https://people.eecs.berkeley.edu/~pabbeel/" target="_blank">Pieter Abbeel</a>,
                    and <a href="http://people.eecs.berkeley.edu/~anca/" target="_blank">Anca Dragan</a>

                  </div>

                  <div class="periodical">

                    <em>Neural Information Processing Systems (NeurIPS) 2020</em>

                  </div>


                  <div class="links">

                    <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>


                    <a href="http://arxiv.org/abs/2006.14796" class="btn btn-sm z-depth-0" role="button"
                      target="_blank">arXiv</a>
                    <a href="https://sites.google.com/berkeley.edu/ave/home" class="btn btn-sm z-depth-0" role="button"
                      target="_blank">Website</a>
                    <a href="https://github.com/yuqingd/empowerment_lander" class="btn btn-sm z-depth-0" role="button"
                      target="_blank">Code</a>
                  </div>

                  <!-- Hidden abstract block -->

                  <div class="abstract hidden">
                    <p>One difficulty in using artificial agents for human-assistive applications lies in the challenge
                      of accurately assisting with a person’s goal(s). Existing methods tend to rely on inferring the
                      human’s goal, which is challenging when there are many potential goals or when the set of
                      candidate goals is difficult to identify. We propose a new paradigm for assistance by instead
                      increasing the human’s ability to control their environment, and formalize this approach by
                      augmenting reinforcement learning with human empowerment. This task-agnostic objective preserves
                      the person’s autonomy and ability to achieve any eventual state. We test our approach against
                      assistance based on goal inference, highlighting scenarios where our method overcomes failure
                      modes stemming from goal ambiguity or misspecification. As existing methods for estimating
                      empowerment in continuous domains are computationally hard, precluding its use in real time
                      learned assistance, we also propose an efficient empowerment-inspired proxy metric. Using this, we
                      are able to successfully demonstrate our method in a shared autonomy user study for a challenging
                      simulated teleoperation task with human-in-the-loop training. </p>
                  </div>

                </div>
              </div>
            </li>

            <li>
              <div class="row">
                <div class="col-sm-4 float-left">

                  <center>
                    <img class="img-fluid z-depth-1 rounded" src="/assets/img/advpop.png">
                  </center>
                </div>

                <div id="vinitsky2020robust" class="col-sm-7">

                  <div class="title"><a href="http://arxiv.org/pdf/2008.01825.pdf">Robust Reinforcement Learning using
                      Adversarial Populations</a></div>
                  <div class="author">
                    <a href="https://eugenevinitsky.github.io/" target="_blank">Eugene Vinitsky*</a>,
                    <em>Yuqing Du*</em>,
                    <a href="http://kanaad.me/" target="_blank">Kanaad Parvate*</a>,
                    <a href="http://kathyjang.com/" target="_blank">Kathy Jang</a>,
                    <a href="https://people.eecs.berkeley.edu/~pabbeel/" target="_blank">Pieter Abbeel</a>,
                    and <a href="https://bayen.berkeley.edu/alex-bayen" target="_blank">Alexandre Bayen</a>
                  </div>

                  <div class="periodical">

                  </div>


                  <div class="links">

                    <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
                    <a href="http://arxiv.org/abs/2008.01825" class="btn btn-sm z-depth-0" role="button"
                      target="_blank">arXiv</a>
                    <a href="https://github.com/eugenevinitsky/robust_RL_multi_adversary" class="btn btn-sm z-depth-0"
                      role="button" target="_blank">Code</a>




                  </div>

                  <!-- Hidden abstract block -->

                  <div class="abstract hidden">
                    <p>Reinforcement Learning (RL) is an effective tool for controller design but can struggle with
                      issues of robustness, failing catastrophically when the underlying system dynamics are perturbed.
                      The Robust RL formulation tackles this by adding worst-case adversarial noise to the dynamics and
                      constructing the noise distribution as the solution to a zero-sum minimax game. However, existing
                      work on learning solutions to the Robust RL formulation has primarily focused on training a single
                      RL agent against a single adversary. In this work, we demonstrate that using a single adversary
                      does not consistently yield robustness to dynamics variations under standard parametrizations of
                      the adversary; the resulting policy is highly exploitable by new adversaries. We propose a
                      population-based augmentation to the Robust RL formulation in which we randomly initialize a
                      population of adversaries and sample from the population uniformly during training. We empirically
                      validate across robotics benchmarks that the use of an adversarial population results in a more
                      robust policy that also improves out-of-distribution generalization. Finally, we demonstrate that
                      this approach provides comparable robustness and generalization as domain randomization on these
                      benchmarks while avoiding a ubiquitous domain randomization failure mode. </p>
                  </div>

                </div>
              </div>
            </li>

            <li>
              <div class="row">
                <div class="col-sm-4 float-left">
                  <center>
                    <img class="img-fluid z-depth-1 rounded" src="/assets/img/groupsurf.png">
                    <!-- <abbr class="badge">ICRA 2019</abbr> -->
                  </center>


                </div>

                <div id="du2019group" class="col-sm-7">

                  <div class="title"><a href="https://arxiv.org/pdf/2104.05933.pdf">Group Surfing: A Pedestrian-Based
                      Approach to Sidewalk Robot Navigation</a></div>
                  <div class="author">

                    <em>Yuqing Du</em>,
                    <a href="https://nickhetherington.ca/" target="_blank">Nicholas J. Hetherington </a>,
                    <a href="https://www.linkedin.com/in/oonchulip97" target="_blank">Chu Lip Oon</a>,

                    <a href="https://research.monash.edu/en/persons/wesley-chan" target="_blank"> Wesley P. Chan</a>,
                    <a href="https://scholar.google.com.au/citations?user=Khbq32QAAAAJ&hl=en" target="_blank">Camilo P.
                      Quintero</a>,

                    <a href="https://research.monash.edu/en/persons/elizabeth-croft" target="_blank">Elizabeth
                      Croft</a>,
                    and <a href="https://mech.ubc.ca/machiel-van-der-loos/" target="_blank">Machiel H. F. Van der
                      Loos</a>

                  </div>

                  <div class="periodical">

                    <em>International Conference on Robotics and Automation (ICRA) 2019</em>


                  </div>


                  <div class="links">

                    <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
                    <a href="https://arxiv.org/abs/2104.05933" class="btn btn-sm z-depth-0" role="button"
                      target="_blank">arXiv</a>

                  </div>

                  <!-- Hidden abstract block -->

                  <div class="abstract hidden">
                    <p>In this paper, we propose a novel navigation system for mobile robots in pedestrian-rich sidewalk
                      environments. Sidewalks are unique in that the pedestrian-shared space has characteristics of both
                      roads and indoor spaces. Like vehicles on roads, pedestrian movement often manifests as linear
                      flows in opposing directions. On the other hand, pedestrians also form crowds and can exhibit much
                      more random movements than vehicles. Classical algorithms are insufficient for safe navigation
                      around pedestrians and remaining on the sidewalk space. Thus, our approach takes advantage of
                      natural human motion to allow a robot to adapt to sidewalk navigation in a safe and
                      socially-compliant manner. We developed a group surfing method which aims to imitate the optimal
                      pedestrian group for bringing the robot closer to its goal. For pedestrian-sparse environments, we
                      propose a sidewalk edge detection and following method. Underlying these two navigation methods,
                      the collision avoidance scheme is human-aware. The integrated navigation stack is evaluated and
                      demonstrated in simulation. A hardware demonstration is also presented.</p>
                  </div>

                </div>
              </div>
            </li>
          </ol>



      </article>

    </div>

  </div>

  <!-- Footer -->


  <footer class="fixed-bottom">
    <div class="container mt-0">
      &copy; Copyright 2024 Yuqing Du.
      Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a
        href="https://github.com/alshedivat/al-folio">al-folio</a> theme. Hosted by <a href="https://pages.github.com/"
        target="_blank">GitHub Pages</a>.


      Last updated: Dec 2024.

    </div>
  </footer>



</body>

<!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js"
  integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg=="
  crossorigin="anonymous"></script>

<!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js"
  integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A=="
  crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js"
  integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ=="
  crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js"
  integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw=="
  crossorigin="anonymous"></script>


<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>






<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>


</html>