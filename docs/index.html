<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>Yuqing  Du</title>
<meta name="description" content="">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<!-- <link rel="stylesheet" href="https://combinatronics.io/jwarby/pygments-css/master/github.css" /> -->

<!-- Styles -->
<link rel="shortcut icon" href="/assets/img/favicon.ico">
<link rel="stylesheet" href="/assets/css/main.css">

<link rel="canonical" href="/">

    
<!-- MathJax -->
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  </head>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light bg-white navbar-expand-sm fixed-top">
    <div class="container">
      
      <div class="row ml-1 ml-sm-0">
        <span class="contact-icon text-center" nav="true">
          <!-- <a href="mailto:%79%69%72%75.%63%68%65%6E@%63%6F%6C%75%6D%62%69%61.%65%64%75"><i class="fas fa-envelope"></i></a> -->
          <a href="https://scholar.google.com/citations?user=j9GCzQUAAAAJ&hl=en" target="_blank" title="Google Scholar"><i class="ai ai-google-scholar"></i></a>
          <a href="https://github.com/yuqingd" target="_blank" title="GitHub"><i class="fab fa-github"></i></a>
          <a href="https://www.linkedin.com/in/yuqingdu" target="_blank" title="LinkedIn"><i class="fab fa-linkedin"></i></a>
          <a href="https://twitter.com/d_yuqing" target="_blank" title="Twitter"><i class="fab fa-twitter"></i></a>
        </span>

      </div>

      <!-- Navbar Toogle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item active">
            <a class="nav-link" href="/">
              home
              
                <span class="sr-only">(current)</span>
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/projects/">
                
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/publications/">
                
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/teaching/">
                
                
              </a>
          </li>
          
          
          
          
          
          
          
          
          
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      <div class="post">

  <header class="post-header">
    <h1 class="post-title">
     <span class="font-weight-bold">Yuqing</span>   Du
    </h1>
  </header>

  <article>
    
    <div class="profile float-right">
      
        <img class="img-fluid z-depth-1 rounded" src="/assets/img/prof_pic.jpg">
      
        <div class="address">
          <p><small>yuqing_du (at) berkeley (dot) edu </small> </p>

        </div>
      
    </div>
    

    <div class="clearfix">
      <p>Hello! I am a Research Scientist at Google DeepMind. I received my PhD from UC Berkeley, advised by Professor <a href="https://people.eecs.berkeley.edu/~pabbeel/">Pieter Abbeel</a> at the <a href="http://bair.berkeley.edu/">Berkeley Artificial Intelligence Research (BAIR) Lab</a>. My thesis, "Human-Centric Reward Design" can be found <a href="https://escholarship.org/content/qt6h96s744/qt6h96s744.pdf">here</a>. </p>
      I have a B.A.Sc in Engineering Physics with a minor in Honours Mathematics from the University of British Columbia, where I was advised by Professors <a href="http://caris-mech.sites.olt.ubc.ca/profiles/machiel-van-der-loos/">Machiel Van der Loos</a> and <a href="https://research.monash.edu/en/persons/elizabeth-croft">Elizabeth Croft</a> at the <a href="https://caris.mech.ubc.ca/">Collaborative Advanced Robotics and Intelligent Systems (CARIS) Lab</a>, where I worked on human-robot interaction.</p>
      <p> Previously, I have also been a visiting researcher at <a href="https://ai.meta.com/research/">FAIR (Meta)</a>, and have interned at <a href="https://www.deepmind.com">DeepMind</a> and <a href="https://x.company">X, the moonshot factory</a>. </p>
      
<p>I am broadly interested in helping agents learn from humans -- whether that be modelling human preferences, learning via interaction, or acquiring world priors from people. For example, some of my prior works 
include training multimodal reward models from human feedback, learning to assist and empower people, and helping agents leverage human priors. Currently, I am interested in developing agents that can learn from diverse sets of human preferences. </p>

<!--
Write your biography here. Tell the world about yourself. Link to your favorite [subreddit](http://reddit.com){:target="\_blank"}. You can put a picture in, too. The code is already in, just name your picture `prof_pic.jpg` and put it in the `img/` folder.

Put your address / P.O. box / other info right below your picture. You can also disable any these elements by editing `profile` property of the YAML header of your `_pages/about.md`. Edit `_bibliography/papers.bib` and Jekyll will render your [publications page](/al-folio/publications/) automatically.

Link to your social media connections, too. This theme is set up to use [Font Awesome icons](http://fortawesome.github.io/Font-Awesome/){:target="\_blank"} and [Academicons](https://jpswalsh.github.io/academicons/){:target="\_blank"}, like the ones below. Add your Facebook, Twitter, LinkedIn, Google Scholar, or just disable all of them. -->

    </div>

    

    
      <div class="publications">

        <h2>preprints</h2>

        <ol class="bibliography"><li><div class="row">
          <div class="col-sm-3 float-left">

            <center>
            <img class="img-fluid z-depth-1 rounded" src="/assets/img/dynalang.png">
          </center> 
          </div>
        
        <div id="lin2023dynalang" class="col-sm-7">
          
        <div class="title"><a href="https://arxiv.org/pdf/2308.01399.pdf">Learning to Model the World with Language</a></div>
          <div class="author">
            <a href="https://www.jessylin.com/">  Jessy Lin</a>, 
            <em>Yuqing Du</em>, 
            <a href="https://www.linkedin.com/in/olivia-watkins-a73580bb/"> Olivia Watkins</a>, 
            <a href="https://danijar.com/">Danijar Hafner</a>, 
            <a href="https://people.eecs.berkeley.edu/~pabbeel/"> Pieter Abbeel</a>,
            <a href="https://people.eecs.berkeley.edu/~klein/"> Dan Klein</a>,  and 
            and <a href="http://people.eecs.berkeley.edu/~anca/" target="_blank">Anca Dragan</a>
        
               
                 
        </div>
        
          <div class="periodical">      
          
          </div>
        
        
        <div class="links">
        
          <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
          <a href="https://arxiv.org/abs/2308.01399" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
          <a href="https://dynalang.github.io" class="btn btn-sm z-depth-0" role="button" target="_blank">Website</a>
          <a href="https://github.com/jlin816/dynalang" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
        
        </div>
        
        <!-- Hidden abstract block -->
        
        <div class="abstract hidden">
          <p>To interact with humans in the world, agents need to understand the diverse types of language that people use, relate them to the visual world, and act based on them. While current agents learn to execute simple language instructions from task rewards, we aim to build agents that leverage diverse language that conveys general knowledge, describes the state of the world, provides interactive feedback, and more. Our key idea is that language helps agents predict the future: what will be observed, how the world will behave, and which situations will be rewarded. This perspective unifies language understanding with future prediction as a powerful self-supervised learning objective. We present Dynalang, an agent that learns a multimodal world model that predicts future text and image representations and learns to act from imagined model rollouts. Unlike traditional agents that use language only to predict actions, Dynalang acquires rich language understanding by using past language also to predict future language, video, and rewards. In addition to learning from online interaction in an environment, Dynalang can be pretrained on datasets of text, video, or both without actions or rewards. From using language hints in grid worlds to navigating photorealistic scans of homes, Dynalang utilizes diverse types of language to improve task performance, including environment descriptions, game rules, and instructions. </p>
        </div>
        
        </div>
        
        
        </div>
        </li></ol>


        <ol class="bibliography"><li><div class="row">
          <div class="col-sm-3 float-left">

            <center>
            <img class="img-fluid z-depth-1 rounded" src="/assets/img/t2i.png">
          </center> 
          </div>
        
        <div id="lee2023t2i" class="col-sm-7">
          
        <div class="title"><a href="https://arxiv.org/pdf/2302.12192.pdf">Aligning Text-to-Image Models using Human Feedback</a></div>
          <div class="author">
            <a href="https://sites.google.com/view/kiminlee">  Kimin Lee</a>, 
            <a href="https://www.haoliu.site"> Hao Liu</a>, 
            <a href="https://www.linkedin.com/in/moonkyung-ryu-b1199918/"> Moonkyung Ryu</a>,
            <a href="https://www.linkedin.com/in/olivia-watkins-a73580bb/"> Olivia Watkins</a>, 
             <em>Yuqing Du</em>, 
             <a href="https://www.cs.toronto.edu/~cebly/"> Craig Boutilier</a>,
             
             <a href="https://people.eecs.berkeley.edu/~pabbeel/"> Pieter Abbeel</a>, 
             <a href="https://mohammadghavamzadeh.github.io"> Mohammad Ghavamzadeh</a>,  and 
             <a href="https://sites.google.com/view/gugurus/home"> Shixiang Shane Gu</a>
        
               
                 
        </div>
        
          <div class="periodical">      
          
          </div>
        
        
        <div class="links">
        
          <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
          <a href="https://arxiv.org/abs/2302.12192" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
        
        </div>
        
        <!-- Hidden abstract block -->
        
        <div class="abstract hidden">
          <p>Deep generative models have shown impressive results in text-to-image synthesis. However, current text-to-image models often generate images that are inadequately aligned with text prompts. We propose a fine-tuning method for aligning such models using human feedback, comprising three stages. First, we collect human feedback assessing model output alignment from a set of diverse text prompts. We then use the human-labeled image-text dataset to train a reward function that predicts human feedback. Lastly, the text-to-image model is fine-tuned by maximizing reward-weighted likelihood to improve image-text alignment. Our method generates objects with specified colors, counts and backgrounds more accurately than the pre-trained model. We also analyze several design choices and find that careful investigations on such design choices are important in balancing the alignment-fidelity tradeoffs. Our results demonstrate the potential for learning from human feedback to significantly improve text-to-image models. </p>
        </div>
        
        </div>
        
        
        </div>
        </li></ol>
        
        <ol class="bibliography"><li><div class="row">
        <div class="col-sm-3 float-left">

          <center>
          <img class="img-fluid z-depth-1 rounded" src="/assets/img/advpop.png">
        </center> 
        </div>
        
        <div id="vinitsky2020robust" class="col-sm-7">
          
            <div class="title"><a href="http://arxiv.org/pdf/2008.01825.pdf">Robust Reinforcement Learning using Adversarial Populations</a></div>
            <div class="author">
              <a href="https://eugenevinitsky.github.io/" target="_blank">Eugene Vinitsky*</a>,
            <em>Yuqing Du*</em>,
              <a href="http://kanaad.me/" target="_blank">Kanaad Parvate*</a>,
              <a href="http://kathyjang.com/" target="_blank">Kathy Jang</a>,
              <a href="https://people.eecs.berkeley.edu/~pabbeel/" target="_blank">Pieter Abbeel</a>,
              and <a href="https://bayen.berkeley.edu/alex-bayen" target="_blank">Alexandre Bayen</a>
            </div>
        
            <div class="periodical">     
            
            </div>
          
        
          <div class="links">
          
            <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
            <a href="http://arxiv.org/abs/2008.01825" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
            <a href="https://github.com/eugenevinitsky/robust_RL_multi_adversary" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
          
          
          
          
          </div>
        
          <!-- Hidden abstract block -->
          
          <div class="abstract hidden">
            <p>Reinforcement Learning (RL) is an effective tool for controller design but can struggle with issues of robustness, failing catastrophically when the underlying system dynamics are perturbed. The Robust RL formulation tackles this by adding worst-case adversarial noise to the dynamics and constructing the noise distribution as the solution to a zero-sum minimax game. However, existing work on learning solutions to the Robust RL formulation has primarily focused on training a single RL agent against a single adversary. In this work, we demonstrate that using a single adversary does not consistently yield robustness to dynamics variations under standard parametrizations of the adversary; the resulting policy is highly exploitable by new adversaries. We propose a population-based augmentation to the Robust RL formulation in which we randomly initialize a population of adversaries and sample from the population uniformly during training. We empirically validate across robotics benchmarks that the use of an adversarial population results in a more robust policy that also improves out-of-distribution generalization. Finally, we demonstrate that this approach provides comparable robustness and generalization as domain randomization on these benchmarks while avoiding a ubiquitous domain randomization failure mode. </p>
          </div>
          
        </div>
        </div>
        </li></ol>
        
 
  <h2>publications</h2>
  <ol class="bibliography">

    <li><div class="row">
      <div class="col-sm-3 float-left">
        <center>
        <img class="img-fluid z-depth-1 rounded" src="/assets/img/dpok.png">
        <!-- <abbr class="badge">CoLLAs 2023</abbr> -->
        </center>
      </div>
    
      <div id="fan2023dpok" class="col-sm-7">
        
        <div class="title"><a href="https://arxiv.org/pdf/2305.16381.pdf">DPOK: Reinforcement Learning for Fine-tuning Text-to-Image Diffusion Models</a></div>
        <div class="author">
          <a href="https://pages.cs.wisc.edu/~yingfan/">Ying Fan</a>, 
          <a href="https://www.linkedin.com/in/olivia-watkins-a73580bb/"> Olivia Watkins</a>, 
          <em>Yuqing Du</em>, 
          <a href="https://www.haoliu.site"> Hao Liu</a>, 
            <a href="https://www.linkedin.com/in/moonkyung-ryu-b1199918/"> Moonkyung Ryu</a>, 
              <a href="https://www.cs.toronto.edu/~cebly/"> Craig Boutilier</a>, 
                <a href="https://people.eecs.berkeley.edu/~pabbeel/"> Pieter Abbeel</a>, 
                  <a href="https://mohammadghavamzadeh.github.io"> Mohammad Ghavamzadeh</a>, 
                    <a href="https://kangwooklee.com"> Kangwook Lee</a>, and
                      <a href="https://sites.google.com/view/kiminlee">  Kimin Lee</a>
      
      </div>
    
          <div class="periodical">
          
            <em>Neural Information Processing Systems (NeurIPS) 2023</em>
          
          </div>
        
    
        <div class="links">
          <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
          <a href="https://arxiv.org/abs/2305.16381" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
          <a href="https://sites.google.com/view/dpok-t2i-diffusion/home" class="btn btn-sm z-depth-0" role="button" target="_blank">Website</a>
        </div>
    
        <!-- Hidden abstract block -->
        
        <div class="abstract hidden">
          <p>Learning from human feedback has been shown to improve text-to-image models. These techniques first learn a reward function that captures what humans care about in the task and then improve the models based on the learned reward function. Even though relatively simple approaches (e.g., rejection sampling based on reward scores) have been investigated, fine-tuning text-to-image models with the reward function remains challenging. In this work, we propose using online reinforcement learning (RL) to fine-tune text-to-image models. We focus on diffusion models, defining the fine-tuning task as an RL problem, and updating the pre-trained text-to-image diffusion models using policy gradient to maximize the feedback-trained reward. Our approach, coined DPOK, integrates policy optimization with KL regularization. We conduct an analysis of KL regularization for both RL fine-tuning and supervised fine-tuning. In our experiments, we show that DPOK is generally superior to supervised fine-tuning with respect to both image-text alignment and image quality. </p>
        </div>
        
      </div>
    </div>
    </li>

    <li><div class="row">
      <div class="col-sm-3 float-left">
        <center>
        <img class="img-fluid z-depth-1 rounded" src="/assets/img/vlm.png">
        <!-- <abbr class="badge">CoLLAs 2023</abbr> -->
        </center>
      </div>
    
      <div id="du2023vlm" class="col-sm-7">
        
          <div class="title"><a href="https://arxiv.org/pdf/2303.07280.pdf">Vision-Language Models as Success Detectors</a></div>
          <div class="author">
            <em>Yuqing Du</em>, 
            <a href="https://ksenia.konyushkova.com">Ksenia Konyushkova</a>, 
            <a href="https://mdenil.com">Misha Denil</a>,
            <a href="https://www.akhilr.com">Akhil Raju</a>,
            Jessica Landon,
            <a href="https://fh295.github.io">Felix Hill</a>, 
            <a href="https://www.cs.ox.ac.uk/people/nando.defreitas/">Nando de Freitas</a>, and
            <a href="https://scholar.google.com/citations?user=l-HhJaUAAAAJ&hl=en">Serkan Cabi</a>
           
          </div>
    
          <div class="periodical">
          
            <em> Conference on Lifelong Learning Agents (CoLLAs) 2023</em> <br><b>(Oral)</b>
          
          </div>
        
    
        <div class="links">
        
          <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
          <a href="https://arxiv.org/abs/2303.07280" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
        </div>
    
        <!-- Hidden abstract block -->
        
        <div class="abstract hidden">
          <p>Detecting successful behaviour is crucial for training intelligent agents. As such, generalisable reward models are a prerequisite for agents that can learn to generalise their behaviour. In this work we focus on developing robust success detectors that leverage large, pretrained vision-language models (Flamingo, Alayrac et al. (2022)) and human reward annotations. Concretely, we treat success detection as a visual question answering (VQA) problem, denoted SuccessVQA. We study success detection across three vastly different domains: (i) interactive language-conditioned agents in a simulated household, (ii) real world robotic manipulation, and (iii) "in-the-wild" human egocentric videos. We investigate the generalisation properties of a Flamingo-based success detection model across unseen language and visual changes in the first two domains, and find that the proposed method is able to outperform bespoke reward models in out-of-distribution test scenarios with either variation. In the last domain of "in-the-wild" human videos, we show that success detection on unseen real videos presents an even more challenging generalisation task warranting future work. We hope our initial results encourage further work in real world success detection and reward modelling.</p>
        </div>
        
      </div>
    </div>
    </li>
    
    <li><div class="row">
      <div class="col-sm-3 float-left">

        <center>
        <img class="img-fluid z-depth-1 rounded" src="/assets/img/ellm.png">
        <!-- <abbr class="badge">ICML 2023</abbr> -->
        </center>
      </div>
    
      <div id="du2023ellm" class="col-sm-7">
        
          <div class="title"><a href="https://arxiv.org/pdf/2302.06692.pdf">Guiding Pretraining in Reinforcement Learning with Large Language Models</a></div>
          <div class="author">
            <em>Yuqing Du*</em>, 
            <a href="https://www.linkedin.com/in/olivia-watkins-a73580bb/" target="_blank">Olivia Watkins*</a>,
            <a href="https://zihanwangki.github.io">Zihan Wang</a>,
            <a href="https://scholar.google.fr/citations?user=VBz8gZ4AAAAJ&hl=fr">Cédric Colas</a>,
            <a href="https://people.eecs.berkeley.edu/~trevor/" target="_blank">Trevor Darrell</a>,
            <a href="https://people.eecs.berkeley.edu/~pabbeel/" target="_blank">Pieter Abbeel</a>,
            <a href="https://abhishekunique.github.io">Abhishek Gupta</a>, and
            <a href="https://www.mit.edu/~jda/">Jacob Andreas</a>
          </div>
    
          <div class="periodical">
          
            <em> International Conference on Machine Learning (ICML) 2023</em>
          
          </div>
        
    
        <div class="links">
        
          <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
          <a href="https://arxiv.org/abs/2302.06692" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
          <a href="https://github.com/yuqingd/ellm" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>

        </div>
    
        <!-- Hidden abstract block -->
        
        <div class="abstract hidden">
          <p>Reinforcement learning algorithms typically struggle in the absence of a dense, well-shaped reward function. Intrinsically motivated exploration methods address this limitation by rewarding agents for visiting novel states or transitions, but these methods offer limited benefits in large environments where most discovered novelty is irrelevant for downstream tasks. We describe a method that uses background knowledge from text corpora to shape exploration. This method, called ELLM (Exploring with LLMs) rewards an agent for achieving goals suggested by a language model prompted with a description of the agent's current state. By leveraging large-scale language model pretraining, ELLM guides agents toward human-meaningful and plausibly useful behaviors without requiring a human in the loop. We evaluate ELLM in the Crafter game environment and the Housekeep robotic simulator, showing that ELLM-trained agents have better coverage of common-sense behaviors during pretraining and usually match or improve performance on a range of downstream tasks.</p>
        </div>
        
      </div>
    </div>
    </li>

  <li><div class="row">
      <div class="col-sm-3 float-left">

        <center>
        <img class="img-fluid z-depth-1 rounded" src="/assets/img/tcl.png">
        <!-- <abbr class="badge">ICRA 2023</abbr> -->
      </center>
      </div>
    
      <div id="khansari2023tcl" class="col-sm-7">
        
          <div class="title"><a href="https://arxiv.org/pdf/2202.01862.pdf">Practical Visual Deep Imitation Learning via Task-Level Domain Consistency</a></div>
          <div class="author">
            <a href="https://scholar.google.ch/citations?user=Z3dxz9IAAAAJ" target="_blank">Mohi Khansari</a>,
            <a href="https://scholar.google.com/citations?hl=en&user=i05Kw5cAAAAJ" target="_blank">Daniel Ho</a>,
           <em>Yuqing Du</em>, 
           Armando Fuentes, 
           Matthew Bennice,
           Nicolas Sievers,
           <a href="https://kirmani.io">Sean Kirmani</a>,
           <a href="https://www.yunfei-bai.com" target="_blank">Yunfei Bai</a>, 
           and  <a href="https://evjang.com" target="_blank">Eric Jang</a>
            
          </div>
    
          <div class="periodical">
          
            <em> International Conference on Robotics and Automation (ICRA) 2023</em>
          
          </div>
        
    
        <div class="links">
        
          <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
          <a href="https://arxiv.org/abs/2202.01862" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
        </div>
    
        <!-- Hidden abstract block -->
        
        <div class="abstract hidden">
          <p>Recent work in visual end-to-end learning for robotics has shown the promise of imitation learning across a variety of tasks. Such approaches are however expensive both because they require large amounts of real world data and rely on time-consuming real-world evaluations to identify the best model for deployment. These challenges can be mitigated by using simulation evaluations to identify high performing policies. However, this introduces the well-known “reality gap” problem, where simulator inaccuracies decorrelate performance in simulation from that of reality. In this paper, we build on top of prior work in GAN-based domain adaptation and introduce the notion of a Task Consistency Loss (TCL), a self-supervised loss that encourages sim and real alignment both at the feature and action-prediction levels. We demonstrate the effectiveness of our approach by teaching a 9-DoF mobile manipulator to perform the challenging task of latched door opening purely from visual inputs such as RGB and depth images. We achieve 69% success across twenty seen and unseen meeting rooms using only ~ 16.2 hours of teleoperated demonstrations in sim and real. To the best of our knowledge, this is the first work to tackle latched door opening from a purely end-to-end learning approach, where the task of navigation and manipulation are jointly modeled by a single neural network.</p>
        </div>
        
      </div>
    </div>
    </li>

    
    <li><div class="row">
      <div class="col-sm-3 float-left">

        <center>
        <img class="img-fluid z-depth-1 rounded" src="/assets/img/vib.png">
        <!-- <abbr class="badge">ICML 2022</abbr> -->
        </center>
      </div>
    
      <div id="du2022vib" class="col-sm-7">
        
          <div class="title"><a href="https://arxiv.org/pdf/2202.07600.pdf">Bayesian Imitation Learning for End-to-End Mobile Manipulation</a></div>
          <div class="author">
                    <em>Yuqing Du</em>,
                      <a href="https://scholar.google.com/citations?hl=en&user=i05Kw5cAAAAJ" target="_blank">Daniel Ho</a>,
                      <a href="https://www.alexalemi.com" target="_blank">Alexander A. Alemi</a>,
                      <a href="https://evjang.com" target="_blank">Eric Jang</a>,
                      and <a href="https://scholar.google.ch/citations?user=Z3dxz9IAAAAJ" target="_blank">Mohi Khansari</a>
            
          </div>
    
          <div class="periodical">
          
            <em>International Conference on Machine Learning (ICML) 2022</em>
          
          </div>
        
    
        <div class="links">
        
          <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
          <a href="https://arxiv.org/abs/2202.07600" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
        </div>
    
        <!-- Hidden abstract block -->
        
        <div class="abstract hidden">
          <p>In this work we investigate and demonstrate benefits of a Bayesian approach to imitation learning from multiple sensor inputs, as applied to the task of opening office doors with a mobile manipulator. Augmenting policies with additional sensor inputs, such as RGB + depth cameras, is a straightforward approach to improving robot perception capabilities, especially for tasks that may favor different sensors in different situations. As we scale multi-sensor robotic learning to unstructured real-world settings (e.g. offices, homes) and more complex robot behaviors, we also increase reliance on simulators for cost, efficiency, and safety. Consequently, the sim-to-real gap across multiple sensor modalities also increases, making simulated validation more difficult. We show that using the Variational Information Bottleneck (Alemi et al., 2016) to regularize convolutional neural networks improves generalization to held-out domains and reduces the sim-to-real gap in a sensor-agnostic manner. As a side effect, the learned embeddings also provide useful estimates of model uncertainty for each sensor. We demonstrate that our method is able to help close the sim-to-real gap and successfully fuse RGB and depth modalities based on understanding of the situational uncertainty of each sensor. In a real-world office environment, we achieve 96% task success, improving upon the baseline by +16%.</p>
        </div>
        
      </div>
    </div>
    </li>




    <li><div class="row">
      <div class="col-sm-3 float-left">

        <center>
        <img class="img-fluid z-depth-1 rounded" src="/assets/img/cusp.png">
        <!-- <abbr class="badge">ICLR 2022</abbr> -->
        </center> 
      </div>
    
      <div id="du2022tango" class="col-sm-7">
        
          <div class="title"><a href="https://arxiv.org/pdf/2202.10608.pdf">It Takes Four to Tango: Multiagent Selfplay for Automatic Curriculum Generation</a></div>
          <div class="author">
                    <em>Yuqing Du</em>,
                      <a href="https://people.eecs.berkeley.edu/~pabbeel/" target="_blank">Pieter Abbeel</a>,
                      and <a href="http://aditya-grover.github.io/" target="_blank">Aditya Grover</a>
            
          </div>
    
          <div class="periodical">
          
            <em>International Conference on Learning Representations (ICLR) 2022</em>
          
          </div>
        
    
        <div class="links">
        
          <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
        
        
          <a href="https://arxiv.org/abs/2202.10608" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
          <a href="https://github.com/yuqingd/cusp" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
        </div>
    
        <!-- Hidden abstract block -->
        
        <div class="abstract hidden">
          <p>We are interested in training general-purpose reinforcement learning agents that can solve a wide variety of goals. Training such agents efficiently requires automatic generation of a goal curriculum. This is challenging as it requires (a) exploring goals of increasing difficulty, while ensuring that the agent (b) is exposed to a diverse set of goals in a sample efficient manner and (c) does not catastrophically forget previously solved goals. We propose Curriculum Self Play (CuSP), an automated goal generation framework that seeks to satisfy these desiderata by virtue of a multi-player game with 4 agents. We extend the asymmetric curricula learning in PAIRED (Dennis et al., 2020) to a symmetrized game that carefully balances cooperation and competition between two off-policy student learners and two regret-maximizing teachers. CuSP additionally introduces entropic goal coverage and accounts for the non-stationary nature of the students, allowing us to automatically induce a curriculum that balances progressive exploration with anti-catastrophic exploitation. We demonstrate that our method succeeds at generating an effective curricula of goals for a range of control tasks, outperforming other methods at zero-shot test-time generalization to novel out-of-distribution goals. </p>
        </div>
        
      </div>
    </div>
    </li>



    <li><div class="row">
      <div class="col-sm-3 float-left">
        
        <center>
        <img class="img-fluid z-depth-1 rounded" src="/assets/img/sim2real.png">
        <!-- <abbr class="badge">ICRA 2021</abbr> -->
        </center> 
      </div>
    
      <div id="du2020sim2real" class="col-sm-7">
        
          <div class="title"><a href="https://arxiv.org/pdf/2104.07662.pdf">Auto-Tuned Sim-to-Real Transfer</a></div>
          <div class="author">
                    <em>Yuqing Du*</em>,
                      <a href="https://www.linkedin.com/in/olivia-watkins-a73580bb/" target="_blank">Olivia Watkins*</a>,
                      <a href="https://people.eecs.berkeley.edu/~trevor/" target="_blank">Trevor Darrell</a>,
                      <a href="https://people.eecs.berkeley.edu/~pabbeel/" target="_blank">Pieter Abbeel</a>,
                      and <a href="https://www.cs.cmu.edu/~dpathak/" target="_blank">Deepak Pathak</a>
            
          </div>
    
          <div class="periodical">
          
            <em>International Conference on Robotics and Automation (ICRA) 2021</em>  <br><b>(Best Cognitive Robotics Paper Finalist)</b>
          
          </div>
        
    
        <div class="links">
        
          <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
        
        
          <a href="https://arxiv.org/abs/2104.07662" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
          <a href="https://yuqingd.github.io/autotuned-sim2real/" class="btn btn-sm z-depth-0" role="button" target="_blank">Website</a>
          <a href="https://github.com/yuqingd/sim2real2sim_rad" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
        </div>
    
        <!-- Hidden abstract block -->
        
        <div class="abstract hidden">
          <p>Policies trained in simulation often fail when transferred to the real world due to the `reality gap' where the simulator is unable to accurately capture the dynamics and visual properties of the real world. Current approaches to tackle this problem, such as domain randomization, require prior knowledge and engineering to determine how much to randomize system parameters in order to learn a policy that is robust to sim-to-real transfer while also not being too conservative. We propose a method for automatically tuning simulator system parameters to match the real world using only raw RGB images of the real world without the need to define rewards or estimate state. Our key insight is to reframe the auto-tuning of parameters as a search problem where we iteratively shift the simulation system parameters to approach the real-world system parameters. We propose a Search Param Model (SPM) that, given a sequence of observations and actions and a set of system parameters, predicts whether the given parameters are higher or lower than the true parameters used to generate the observations. We evaluate our method on multiple robotic control tasks in both sim-to-sim and sim-to-real transfer, demonstrating significant improvement over naive domain randomization.  </p>
        </div>
        
      </div>
    </div>
    </li>
<li><div class="row">
  <div class="col-sm-3 float-left">

    <center>
    <img class="img-fluid z-depth-1 rounded" src="/assets/img/empowerment.png">
    <!-- <abbr class="badge">NeurIPS 2020</abbr> -->
    </center> 
  </div>

  <div id="du2020ave" class="col-sm-7">
    
      <div class="title"><a href="http://arxiv.org/pdf/2006.14796.pdf">AvE: Assistance via Empowerment</a></div>
      <div class="author">
                <em>Yuqing Du</em>,
                  <a href="https://www.researchgate.net/profile/Stas_Tiomkin" target="_blank">Stas Tiomkin</a>,
                  <a href="https://www.microsoft.com/en-us/research/people/emrek/" target="_blank">Emre Kiciman</a>,
                  <a href="https://scholar.google.com/citations?user=wdZT8N8AAAAJ&hl=en" target="_blank">Daniel Polani</a>,
                  <a href="https://people.eecs.berkeley.edu/~pabbeel/" target="_blank">Pieter Abbeel</a>,
                  and <a href="http://people.eecs.berkeley.edu/~anca/" target="_blank">Anca Dragan</a>
        
      </div>

      <div class="periodical">
      
        <em>Neural Information Processing Systems (NeurIPS) 2020</em>
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
      <a href="http://arxiv.org/abs/2006.14796" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
      <a href="https://sites.google.com/berkeley.edu/ave/home" class="btn btn-sm z-depth-0" role="button" target="_blank">Website</a>
      <a href="https://github.com/yuqingd/empowerment_lander" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>One difficulty in using artificial agents for human-assistive applications lies in the challenge of accurately assisting with a person’s goal(s). Existing methods tend to rely on inferring the human’s goal, which is challenging when there are many potential goals or when the set of candidate goals is difficult to identify. We propose a new paradigm for assistance by instead increasing the human’s ability to control their environment, and formalize this approach by augmenting reinforcement learning with human empowerment. This task-agnostic objective preserves the person’s autonomy and ability to achieve any eventual state. We test our approach against assistance based on goal inference, highlighting scenarios where our method overcomes failure modes stemming from goal ambiguity or misspecification. As existing methods for estimating empowerment in continuous domains are computationally hard, precluding its use in real time learned assistance, we also propose an efficient empowerment-inspired proxy metric. Using this, we are able to successfully demonstrate our method in a shared autonomy user study for a challenging simulated teleoperation task with human-in-the-loop training.  </p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-3 float-left">
    <center>
    <img class="img-fluid z-depth-1 rounded" src="/assets/img/groupsurf.png">
    <!-- <abbr class="badge">ICRA 2019</abbr> -->
  </center> 
    
  
  </div>

  <div id="du2019group" class="col-sm-7">
    
      <div class="title"><a href="https://arxiv.org/pdf/2104.05933.pdf">Group Surfing: A Pedestrian-Based Approach to Sidewalk Robot Navigation</a></div>
      <div class="author">
        
                <em>Yuqing Du</em>,
                  <a href="https://nickhetherington.ca/" target="_blank">Nicholas J. Hetherington </a>,
                  <a href="https://www.linkedin.com/in/oonchulip97" target="_blank">Chu Lip Oon</a>,
                
                  <a href="https://research.monash.edu/en/persons/wesley-chan" target="_blank"> Wesley P. Chan</a>,
                  <a href="https://scholar.google.com.au/citations?user=Khbq32QAAAAJ&hl=en" target="_blank">Camilo P. Quintero</a>,
                
                  <a href="https://research.monash.edu/en/persons/elizabeth-croft" target="_blank">Elizabeth Croft</a>,
                  and <a href="https://mech.ubc.ca/machiel-van-der-loos/" target="_blank">Machiel H. F. Van der Loos</a>
                
      </div>

      <div class="periodical">
      
        <em>International Conference on Robotics and Automation (ICRA) 2019</em>
        
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
      <a href="https://arxiv.org/abs/2104.05933" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>In this paper, we propose a novel navigation system for mobile robots in pedestrian-rich sidewalk environments. Sidewalks are unique in that the pedestrian-shared space has characteristics of both roads and indoor spaces. Like vehicles on roads, pedestrian movement often manifests as linear flows in opposing directions. On the other hand, pedestrians also form crowds and can exhibit much more random movements than vehicles. Classical algorithms are insufficient for safe navigation around pedestrians and remaining on the sidewalk space. Thus, our approach takes advantage of natural human motion to allow a robot to adapt to sidewalk navigation in a safe and socially-compliant manner. We developed a group surfing method which aims to imitate the optimal pedestrian group for bringing the robot closer to its goal. For pedestrian-sparse environments, we propose a sidewalk edge detection and following method. Underlying these two navigation methods, the collision avoidance scheme is human-aware. The integrated navigation stack is evaluated and demonstrated in simulation. A hardware demonstration is also presented.</p>
    </div>
    
  </div>
</div>
</li></ol>
    

    
  </article>

</div>

    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  <div class="container mt-0">
    &copy; Copyright 2023 Yuqing Du.
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>.

    
    Last updated: July 2023.
    
  </div>
</footer>



  </body>

  <!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  



<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>


</html>
